{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfd81b34",
   "metadata": {},
   "source": [
    "\n",
    "# EDA and regression using Sklearn (Boston Dataset)\n",
    "\n",
    "Prepared by:\n",
    "\n",
    "<b> Noman H chowdhury </b>\n",
    "\n",
    "Reference for theoretical content:\n",
    "\n",
    "Chapater 3 ISLP https://www.statlearning.com/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e6256f",
   "metadata": {},
   "source": [
    "### Regression - only SKlearn with EDA (Boston dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b18c1628",
   "metadata": {
    "execution": {},
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# test line to chk github\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f965f608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fff2f42",
   "metadata": {},
   "source": [
    "We  will use the `Boston` housing data set.  The `Boston` dataset records  `medv`  (median house value) for $506$ neighborhoods\n",
    "around Boston.  We will build a regression model to predict  `medv`  using $13$\n",
    "predictors such as  `rmvar`  (average number of rooms per house),\n",
    " `age`  (proportion of owner-occupied units built prior to 1940), and  `lstat`  (percent of\n",
    "households with low socioeconomic status).  We will use `statsmodels` for this\n",
    "task, a `Python` package that implements several commonly used\n",
    "regression methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e9a7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"path of working directory\")\n",
    "os.getcwd()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9371ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Boston = pd.read_csv(\"Boston.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c2a97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in each column\n",
    "missing_values = Boston.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb2af2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the percentage of missing values for each column\n",
    "missing_percentage = Boston.isnull().mean() * 100\n",
    "print(missing_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c424a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing numerical values with the mean\n",
    "Boston['lstat'] = Boston['lstat'].fillna(Boston['lstat'].mean())\n",
    "\n",
    "# For categorical data, you might use the mode\n",
    "# Boston['categorical_column'] = Boston['categorical_column'].fillna(Boston['categorical_column'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23e12351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also Drop columns with a high percentage of missing values\n",
    "# Boston = Boston.drop(columns=['column_to_drop'])\n",
    "\n",
    "# Drop rows with any missing values\n",
    "Boston = Boston.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311d2457",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Basic Statistics\n",
    "print(Boston.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0522942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for numerical features\n",
    "Boston.hist(bins=15, figsize=(15, 10))\n",
    "plt.suptitle(\"Histograms of Numerical Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1758ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot for numerical features to identify outliers\n",
    "Boston.plot(kind='box', subplots=True, layout=(4,4), figsize=(15, 10))\n",
    "plt.suptitle(\"Boxplots of Numerical Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22bf0177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier handling Example: Remove outliers beyond 3 standard deviations from the mean\n",
    "from scipy import stats\n",
    "Boston_cleaned = Boston[(np.abs(stats.zscore(Boston)) < 3).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c2ce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "correlation_matrix = Boston.corr()\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3779e53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Heatmap visualization of the correlation matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=.5)\n",
    "plt.title(\"Correlation Matrix of Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa671d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pair Plot for Selected Features\n",
    "\n",
    "# Selecting a subset of features for pair plot\n",
    "subset_features = ['rm', 'age', 'lstat', 'medv']\n",
    "#subset_features = ['medv', 'lstat', 'rm',\\\n",
    "#    'ptratio', 'age']\n",
    "\n",
    "# Pair plot\n",
    "#sns.pairplot(Boston[subset_features])\n",
    "sns.pairplot(Boston[subset_features], kind='reg')\n",
    "plt.suptitle(\"Pair Plot of Selected Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a7343b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation with the target variable (medv):\n",
      "medv       1.000000\n",
      "rm         0.695360\n",
      "zn         0.360445\n",
      "dis        0.249929\n",
      "chas       0.175260\n",
      "age       -0.376955\n",
      "rad       -0.381626\n",
      "crim      -0.388305\n",
      "nox       -0.427321\n",
      "tax       -0.468536\n",
      "indus     -0.483725\n",
      "ptratio   -0.507787\n",
      "lstat     -0.737231\n",
      "Name: medv, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Correlation with the target variable 'medv'\n",
    "correlation_with_target = Boston.corr()['medv'].sort_values(ascending=False)\n",
    "print(\"Correlation with the target variable (medv):\")\n",
    "print(correlation_with_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf891a2",
   "metadata": {},
   "source": [
    "### Profiling - automatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c66d2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ydata-profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f95bec",
   "metadata": {},
   "source": [
    "We can use the ydata-profiling library, which is a powerful tool for generating comprehensive reports on your data, including statistics, correlations, and even the identification of missing values. It's particularly useful at the exploratory data analysis (EDA) stage of your project because it provides a quick and deep insight into the dataset with minimal code. This can help you make informed decisions on how to handle missing data, among other preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c7c430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "# Generate the profile report\n",
    "profile = ProfileReport(Boston, title='Pandas Profiling Report', explorative=True)\n",
    "\n",
    "# To display the report in a Jupyter notebook\n",
    "profile.to_notebook_iframe()\n",
    "\n",
    "# To save the report to a file\n",
    "# profile.to_file(\"boston_data_profile.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a511ee83",
   "metadata": {},
   "source": [
    "If you're working with particularly large datasets, pandas-profiling can be resource-intensive and may take some time to generate the report. In such cases, it's possible to customize the report generation to skip certain analyses or to increase efficiency by adjusting the minimal=True parameter in the ProfileReport function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eaa9c1",
   "metadata": {},
   "source": [
    "## Now, Regression with sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9530ac32",
   "metadata": {},
   "source": [
    "In scikit-learn, there isn't a built-in method equivalent to the summary output you might be familiar with from statistical packages like R's lm() function or Python's statsmodels. However, you can manually compute and display key statistics for your regression model, such as R-squared, adjusted R-squared, coefficients, standard errors, p-values, and the confidence intervals for the coefficients.\n",
    "\n",
    "For a simple linear regression model using scikit-learn, you can manually calculate and display some of these statistics, but for a more detailed summary, including p-values and confidence intervals, you would typically need to use the statsmodels library.\n",
    "\n",
    "Here's a basic example of how to get a summary of key statistics from a scikit-learn linear regression model, followed by a more detailed example using statsmodels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127932cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0626ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using scikit-learn to Display Basic Model Summaryimport pandas as pd\n",
    "# Interesting that installation name and import name are different\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression # we could also use sklearn.LinearRegression (??)\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fcc052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select your features and target\n",
    "\n",
    "X = Boston[['lstat']] # or X = Boston['lstat'].values.reshape(-1,1); # double brackets for X to be a dataframe\n",
    "Y = Boston['medv']\n",
    "# Optional: Split the data into training and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and fit the regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, Y)\n",
    "#print(model_fitted_sk.summary()) # This will not work as there is no summary method in sklearn; whereas statsmodels has a summary method (print(model_fitted_sm.summary() works)\n",
    "print(\"Model coefs:\", model.coef_) # however we can get the coefficients\n",
    "# print(f'Coefficient for lstat: {model.coef_[0]}') # and the coefficient for lstat\n",
    "print(\"Model Intercept: \",model.intercept_) # and the intercept\n",
    "\n",
    "#print(model_fitted_sk.summary()) # this works for statsmodels but not for sklearn\n",
    "\n",
    "# Predictions and evaluation\n",
    "Y_pred = model.predict(X) # or X_train\n",
    "mse = mean_squared_error(Y, Y_pred) # or y_train\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "#print(\"R-squared:\", model.score(X, Y)) # or X_train, y_train\n",
    "print(\"R-squared:\", r2_score(Y,Y_pred)) # CAUTION: This is the same as the previous line but they take different arguments (Y, Y_pred) vs. (X, Y)\n",
    "print(\"Adjusted R-squared:\", 1 - (1-model.score(X, Y))*(len(Y)-1)/(len(Y)-X.shape[1]-1)) # or X_train, y_train\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = Y - Y_pred\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(Y_pred, residuals, color='blue', s=50, alpha=0.6)\n",
    "plt.hlines(y=0, xmin=min(Y_pred), xmax=max(Y_pred), colors='red', linestyles='--')\n",
    "plt.xlabel('Fitted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Fitted Values vs. Residuals: Model 0')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68224e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1 with training and test data with only one feature\n",
    "\n",
    "X = Boston[['lstat']] # or X = Boston['lstat'].values.reshape(-1,1); # double brackets for X to be a dataframe\n",
    "Y = Boston['medv'] # or Y = Boston.medv, did not put in double brackets as it is a series\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and fit the regression model on the training set\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions on both the training and test sets\n",
    "Y_train_pred = model.predict(X_train)\n",
    "Y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print the MSE for both\n",
    "mse_train = mean_squared_error(Y_train, Y_train_pred)\n",
    "mse_test = mean_squared_error(Y_test, Y_test_pred)\n",
    "\n",
    "# Calculate and print the R-squared for both\n",
    "r2_train = r2_score(Y_train, Y_train_pred)\n",
    "r2_test = r2_score(Y_test, Y_test_pred)\n",
    "\n",
    "print(f'Model 1 - Training MSE: {mse_train}, Test MSE: {mse_test}')\n",
    "print(f'Model 1 - Training R^2: {r2_train}, Test R^2: {r2_test}')\n",
    "\n",
    "# Print the model coefficients and intercept\n",
    "print(f'Intercept: {model.intercept_}')\n",
    "print(f'Coefficient for lstat: {model.coef_[0]}')\n",
    "print('All Coefficients:', model.coef_)\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = Y_train - Y_train_pred\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(Y_train_pred, residuals, color='blue', s=50, alpha=0.6)\n",
    "plt.hlines(y=0, xmin=min(Y_train_pred), xmax=max(Y_train_pred), colors='red', linestyles='--')\n",
    "plt.xlabel('Fitted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Fitted Values vs. Residuals: Model 1')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17dacc1",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625b78cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Including 'lstat' and 'age'\n",
    "\n",
    "X = Boston[['lstat', 'age']]\n",
    "Y = Boston['medv'] # or Y = Boston.medv, did not put in double brackets as it is a series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf05fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: Using all available predictors\n",
    "\n",
    "X = Boston.drop('medv', axis=1)  # Drop the target to use all other columns as predictors\n",
    "Y = Boston['medv']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b811715e",
   "metadata": {},
   "source": [
    "## Non-linear Transformations of the Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ed6b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4: Add lstat^2 to the dataset\n",
    "\n",
    "Boston['lstat_squared'] = Boston['lstat']**2\n",
    "X = Boston[['lstat', 'lstat_squared']]\n",
    "Y = Boston['medv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb5dde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 5: Add lstat^2 and age to the dataset\n",
    "\n",
    "Boston['lstat_squared'] = Boston['lstat']**2\n",
    "X = Boston[['lstat', 'lstat_squared', 'age']]\n",
    "Y = Boston['medv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a14586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL -> Model 6 (OPTIONAL if you are not comfortable with interaction terms): Add an interaction term between 'lstat' and 'age'\n",
    "\n",
    "# Create the interaction term and add it to the dataset\n",
    "\n",
    "#Boston['lstat_squared'] = Boston['lstat']**2\n",
    "Boston['lstat_age_interaction'] = Boston['lstat'] * Boston['age']\n",
    "X = Boston[['lstat', 'age', 'lstat_age_interaction']]\n",
    "Y = Boston['medv']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6123a191",
   "metadata": {},
   "source": [
    "## Qualitative Predictors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983d95d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 7: Using 'lstat' and 'chas'\n",
    "X = Boston[['lstat', 'chas']]\n",
    "Y = Boston['medv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8967e02e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3656266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL -> Model 8: Using 'age' and one-hot encoded 'lstat' categories (OPTIONAL: just for demonstration how to convert a categorical variable to numerical)\n",
    "\n",
    "# Define bins and labels\n",
    "bins = [0, 15, 30, float('inf')]\n",
    "labels = ['low', 'medium', 'high']\n",
    "\n",
    "# Bin the 'lstat' variable\n",
    "Boston['lstat_cat'] = pd.cut(Boston['lstat'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Check the new categorical variable\n",
    "print(Boston['lstat_cat'].value_counts())\n",
    "\n",
    "\n",
    "# One-hot encode 'lstat_cat'\n",
    "lstat_dummies = pd.get_dummies(Boston['lstat_cat'], prefix='lstat')\n",
    "\n",
    "# Join the encoded DataFrame with the original DataFrame (ensure to avoid duplicating the 'medv' column)\n",
    "Boston_encoded = Boston.join(lstat_dummies).drop(['lstat_cat'], axis=1)\n",
    "\n",
    "# Preview to ensure encoding worked as expected\n",
    "print(Boston_encoded.head())\n",
    "\n",
    "# Select predictors - 'age' and one-hot encoded 'lstat' categories, excluding one category\n",
    "X = Boston_encoded[['age', 'lstat_medium', 'lstat_high']]  # Dropped 'lstat_low'\n",
    "Y = Boston_encoded['medv']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f488d3c3",
   "metadata": {},
   "source": [
    "## KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a08600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming 'Boston' DataFrame is already loaded\n",
    "X = Boston[['lstat']]\n",
    "Y = Boston['medv']\n",
    "\n",
    "# For some models like KNN, we need to do feature scaling\n",
    "scaler = StandardScaler(with_mean=True, with_std=True, copy=True)\n",
    "scaler.fit(X)\n",
    "X_std = scaler.transform(X)\n",
    "\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_std, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and fit the KNN regression model on the training set\n",
    "# Let's arbitrarily choose K=5\n",
    "k = 5\n",
    "model_knn = KNeighborsRegressor(n_neighbors=k)\n",
    "model_knn.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions on both the training and test sets\n",
    "Y_train_pred = model_knn.predict(X_train)\n",
    "Y_test_pred = model_knn.predict(X_test)\n",
    "\n",
    "# Calculate and print the MSE for both\n",
    "mse_train_knn = mean_squared_error(Y_train, Y_train_pred)\n",
    "mse_test_knn = mean_squared_error(Y_test, Y_test_pred)\n",
    "\n",
    "# Calculate and print the R-squared for both\n",
    "r2_train_knn = r2_score(Y_train, Y_train_pred)\n",
    "r2_test_knn = r2_score(Y_test, Y_test_pred)\n",
    "\n",
    "print(f'Model KNN with k={k} - Training MSE: {mse_train_knn}, Test MSE: {mse_test_knn}')\n",
    "print(f'Model KNN with k={k} - Training R^2: {r2_train_knn}, Test R^2: {r2_test_knn}')\n",
    "\n",
    "# Calculate residuals for training data to plot\n",
    "residuals_knn = Y_train - Y_train_pred\n",
    "\n",
    "# Plot for KNN Model\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(Y_train_pred, residuals_knn, color='blue', s=50, alpha=0.6)\n",
    "plt.hlines(y=0, xmin=min(Y_train_pred), xmax=max(Y_train_pred), colors='red', linestyles='--')\n",
    "plt.xlabel('Fitted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title(f'Fitted Values vs. Residuals: Model KNN with k={k}')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "Rmd,ipynb",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
